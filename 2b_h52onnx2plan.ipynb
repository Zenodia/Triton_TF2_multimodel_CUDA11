{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.1.2.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "trt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./model_repository/custom_plan/1/',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2onnx\n",
      "  Downloading tf2onnx-1.6.3-py3-none-any.whl (185 kB)\n",
      "\u001b[K     |████████████████████████████████| 185 kB 19.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting onnx>=1.4.1\n",
      "  Downloading onnx-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4 MB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from tf2onnx) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.1 in /usr/local/lib/python3.6/dist-packages (from tf2onnx) (1.17.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tf2onnx) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx>=1.4.1->tf2onnx) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx>=1.4.1->tf2onnx) (3.7.4.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx>=1.4.1->tf2onnx) (49.6.0)\n",
      "Installing collected packages: onnx, tf2onnx\n",
      "Successfully installed onnx-1.7.0 tf2onnx-1.6.3\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-24 16:02:04.625489: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-08-24 16:02:05,959 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2020-08-24 16:02:05.975260: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: /usr/lib/x86_64-linux-gnu/libcuda.so.1: file too short; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-08-24 16:02:05.975283: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-24 16:02:05.975348: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2020-08-24 16:02:06.009264: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2194990000 Hz\n",
      "2020-08-24 16:02:06.015654: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8a84000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-24 16:02:06.015682: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-08-24 16:02:06,518 - INFO - Signatures found in model: [serving_default].\n",
      "2020-08-24 16:02:06,518 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2020-08-24 16:02:06.520537: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2020-08-24 16:02:06.520678: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2020-08-24 16:02:06.530229: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:824] Optimization results for grappler item: graph_to_optimize\n",
      "2020-08-24 16:02:06.530251: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:826]   function_optimizer: Graph size after: 67 nodes (52), 93 edges (78), time = 2.102ms.\n",
      "2020-08-24 16:02:06.530257: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:826]   function_optimizer: function_optimizer did nothing. time = 0.041ms.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf2onnx/tf_loader.py:368: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2020-08-24 16:02:06,906 - WARNING - From /usr/local/lib/python3.6/dist-packages/tf2onnx/tf_loader.py:368: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2020-08-24 16:02:06.910023: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2020-08-24 16:02:06.910188: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2020-08-24 16:02:07.064666: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:824] Optimization results for grappler item: graph_to_optimize\n",
      "2020-08-24 16:02:07.064707: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:826]   constant_folding: Graph size after: 55 nodes (-12), 69 edges (-24), time = 101.439ms.\n",
      "2020-08-24 16:02:07.064731: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:826]   function_optimizer: function_optimizer did nothing. time = 0.069ms.\n",
      "2020-08-24 16:02:07.064736: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:826]   constant_folding: Graph size after: 55 nodes (0), 69 edges (0), time = 22.192ms.\n",
      "2020-08-24 16:02:07.064745: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:826]   function_optimizer: function_optimizer did nothing. time = 0.074ms.\n",
      "2020-08-24 16:02:07,224 - INFO - Using tensorflow=2.2.0, onnx=1.7.0, tf2onnx=1.6.3/d4abc8\n",
      "2020-08-24 16:02:07,224 - INFO - Using opset <onnx, 8>\n",
      "2020-08-24 16:02:09,273 - INFO - Optimizing ONNX model\n",
      "2020-08-24 16:02:09,358 - INFO - After optimization: Cast -1 (1->0), Identity -5 (5->0), Transpose -14 (16->2)\n",
      "2020-08-24 16:02:09,362 - INFO - \n",
      "2020-08-24 16:02:09,362 - INFO - Successfully converted TensorFlow model ./model_ckpt to ONNX\n",
      "2020-08-24 16:02:09,398 - INFO - ONNX model is saved at model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model ./model_ckpt --output model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile engine.py \n",
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "def build_engine(onnx_path, shape = [1,224,224,3]):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the function to create the TensorRT engine\n",
    "    Args:\n",
    "      onnx_path : Path to onnx_file. \n",
    "      shape : Shape of the input of the ONNX file. \n",
    "    \"\"\"\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1) as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        builder.max_workspace_size = (1 << 30)\n",
    "        with open(onnx_path, 'rb') as model:\n",
    "            parser.parse(model.read())\n",
    "        network.get_input(0).shape = shape\n",
    "        engine = builder.build_cuda_engine(network)\n",
    "        return engine\n",
    "\n",
    "def save_engine(engine, file_name):\n",
    "    buf = engine.serialize()\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(buf)\n",
    "def load_engine(trt_runtime, engine_path):\n",
    "    with open(engine_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine as eng\n",
    "import argparse\n",
    "from onnx import ModelProto \n",
    "import tensorrt as trt\n",
    "\n",
    "engine_name = \"model.plan\"\n",
    "onnx_path = \"model.onnx\"\n",
    "batch_size = 1 \n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "\n",
    "model = ModelProto()\n",
    "with open(onnx_path, \"rb\") as f:\n",
    "    model.ParseFromString(f.read())\n",
    "\n",
    "d0 = model.graph.input[0].type.tensor_type.shape.dim[1].dim_value\n",
    "d1 = model.graph.input[0].type.tensor_type.shape.dim[2].dim_value\n",
    "d2 = model.graph.input[0].type.tensor_type.shape.dim[3].dim_value\n",
    "shape = [batch_size , d0, d1 ,d2]\n",
    "engine = eng.build_engine(onnx_path, shape= shape)\n",
    "eng.save_engine(engine, engine_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "def allocate_buffers(engine, batch_size, data_type):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the function to allocate buffers for input and output in the device\n",
    "    Args:\n",
    "      engine : The path to the TensorRT engine. \n",
    "      batch_size : The batch size for execution time.\n",
    "      data_type: The type of the data for input and output, for example trt.float32. \n",
    "\n",
    "    Output:\n",
    "      h_input_1: Input in the host.\n",
    "      d_input_1: Input in the device. \n",
    "      h_output_1: Output in the host. \n",
    "      d_output_1: Output in the device. \n",
    "      stream: CUDA stream.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine dimensions and create page-locked memory buffers (which won't be swapped to disk) to hold host inputs/outputs.\n",
    "    h_input_1 = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(data_type))\n",
    "    h_output = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(data_type))\n",
    "    # Allocate device memory for inputs and outputs.\n",
    "    d_input_1 = cuda.mem_alloc(h_input_1.nbytes)\n",
    "\n",
    "    d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "    # Create a stream in which to copy inputs/outputs and run inference.\n",
    "    stream = cuda.Stream()\n",
    "    return h_input_1, d_input_1, h_output, d_output, stream \n",
    "\n",
    "def load_images_to_buffer(pics, pagelocked_buffer):\n",
    "\n",
    "    preprocessed = np.asarray(pics).ravel()\n",
    "    np.copyto(pagelocked_buffer, preprocessed)\n",
    "\n",
    "\n",
    "def do_inference(engine, pics_1, h_input_1, d_input_1, h_output, d_output, stream, batch_size, height, width):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the function to run the inference\n",
    "    Args:\n",
    "      engine : Path to the TensorRT engine. \n",
    "      pics_1 : Input images to the model.  \n",
    "      h_input_1: Input in the host. \n",
    "      d_input_1: Input in the device. \n",
    "      h_output_1: Output in the host. \n",
    "      d_output_1: Output in the device. \n",
    "      stream: CUDA stream.\n",
    "      batch_size : Batch size for execution time.\n",
    "      height: Height of the output image.\n",
    "      width: Width of the output image.\n",
    "\n",
    "    Output:\n",
    "      The list of output images.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    load_images_to_buffer(pics_1, h_input_1)\n",
    "\n",
    "    with engine.create_execution_context() as context:\n",
    "        # Transfer input data to the GPU.\n",
    "        cuda.memcpy_htod_async(d_input_1, h_input_1, stream)\n",
    "\n",
    "        # Run inference.\n",
    "\n",
    "        context.profiler = trt.Profiler()\n",
    "        context.execute(batch_size=1, bindings=[int(d_input_1), int(d_output)])\n",
    "\n",
    "        # Transfer predictions back from the GPU.\n",
    "        cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "        # Synchronize the stream.\n",
    "        stream.synchronize()\n",
    "        # Return the host output.\n",
    "        out = h_output.reshape((batch_size,1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import engine as eng\n",
    "import inference as inf\n",
    "import skimage\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pycuda\n",
    "import pycuda.autoinit\n",
    "## construct pre_processing for testing model prediction \n",
    "def inference_processing(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize((224, 224), Image.BILINEAR)\n",
    "    img = np.array(img)\n",
    "\n",
    "    img=img/255\n",
    "    return img.reshape(1,224,224,3)\n",
    "def get_prediction(pred):\n",
    "    \n",
    "    pred=np.argmax(pred)\n",
    "    return np.int32(pred)\n",
    "outputlayer_name = \"outputs\"\n",
    "input_file_path = \"parkinson.png\"\n",
    "onnx_file = \"model.onnx\"\n",
    "serialized_plan_fp32 = \"model.plan\"\n",
    "CHANNEL = 3\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "\n",
    "\n",
    "im = inference_processing(input_file_path)\n",
    "\n",
    "engine = eng.load_engine(trt_runtime, serialized_plan_fp32)\n",
    "h_input, d_input, h_output, d_output, stream = inf.allocate_buffers(engine, 1, trt.float32)\n",
    "out = inf.do_inference(engine, im, h_input, d_input, h_output, d_output, stream, 1, HEIGHT, WIDTH)\n",
    "print(out.shape), get_prediction(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model_repository/custom_plan/config.pbtxt\n",
    "name: \"custom_plan\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size : 1\n",
    "input [\n",
    "  {\n",
    "    name: \"hand_drawing:0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NHWC\n",
    "    dims: [ 224,224 ,3]\n",
    "    reshape {shape : [1,224,224,3]}\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"Identity:0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "    reshape{shape:[1,1]}\n",
    "    label_filename: \"labels.txt\"\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 2\n",
    "    kind: KIND_GPU\n",
    "  }\n",
    "]\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 1 ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model_repository/custom_plan/labels.txt \n",
    "healthy\n",
    "parkinson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp model.plan ./model_repository/custom_plan/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.plan']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('./model_repository/custom_plan/1/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
